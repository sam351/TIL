# [실습] 지도 위에 marker 삽입하기
df <- round(data.frame(x=jitter(rep(-95.36, 25), amount=.3),
y=jitter(rep(29.76, 25), amount=.3) ), digits=2)
map <- get_googlemap('houston', markers=df, path=df, scale=2)
ggmap(map, extent = 'device')
# 4.2 종합지도 서비스 관련 API 이용
# [실습] roadmap 타입으로 지도 그리기
map <- get_map(location ="london", zoom=14, maptype='roadmap', scale=2)
# get_map("중심지역", 확대비율, 지도유형) : ggmap에서 제공하는 함수
ggmap(map, size=c(600,600), extent='device')
# [실습] watercolor 타입으로 지도 그리기
map <- get_map(location ="seoul", zoom=8, maptype='watercolor', scale=2)
ggmap(map, size=c(600,600), extent='device')
# 4.3 지도 이미지에 레이어 적용
# [실습] 서울지역 4년제 대학교 위치 표시
university <- read.csv("C:/Rwork/Part-II/university.csv",header=T)
university # # 학교명","LAT","LON"
# 레이어1 : 정적 지도 생성
kor <- get_map('seoul', zoom = 11, maptype = 'watercolor')
ggmap(kor)
# 레이어2 : 지도위에 포인트 추가
ggmap(kor)+geom_point(data=university, aes(x=LON, y=LAT,color=factor(학교명)),size=3)
kor.map <- ggmap(kor)+geom_point(data=university, aes(x=LON, y=LAT,color=factor(학교명)),size=3)
# 레이어3 : 지도위에 텍스트 추가
kor.map + geom_text(data=university, aes(x=LON+0.01, y=LAT+0.01,label=학교명),size=5)
# [실습] 지도 저장하기
ggsave("C:/Rwork/output/university1.png",width=10.24,height=7.68)
# 밀도 적용 파일 저장
ggsave("C:/Rwork/output/university2.png",dpi=1000) # 9.21 x 7.68 in image
#  4.4 공간 지도 시각화 실습
# [실습] 다양한 지도 유형
# maptype='terrain'
map1 <- get_map("daegu", zoom=7 ,  maptype='terrain')
map2 <- ggmap(map1)
map3 <- map2 + geom_point(aes(x=lon,y=lat,colour=house,size=house),data=df)
map3 + geom_text(data=df, aes(x=lon+0.01, y=lat+0.18,label=region),size=3)
# maptype='satellite'
map1 <- get_map("daegu", zoom=7 ,  maptype='satellite')
map2 <- ggmap(map1)
map3 <- map2 + geom_point(aes(x=lon,y=lat,colour=house,size=house),data=df)
map3 + geom_text(data=df, aes(x=lon+0.01,y=lat+0.18,colour=region,label=region),size=3)
# maptype='roadmap'
map1 <- get_map("daegu", zoom=7 ,  maptype='roadmap')
map2 <- ggmap(map1)
map3 <- map2 + geom_point(aes(x=lon,y=lat,colour=house,size=house),data=df)
map3 + geom_text(data=df, aes(x=lon+0.01, y=lat+0.18,label=region),size=3)
# maptype='hybrid'
map1 <- get_map("jeonju", zoom=7,  maptype='hybrid')
map2 <- ggmap(map1)
map3 <- map2 + geom_point(aes(x=lon,y=lat,colour=house,size=house),data=df)
map3 + geom_text(data=df, aes(x=lon+0.01, y=lat+0.18,label=region),size=3)
map3 + geom_density2d()
# [실습] 다양한 지도 유형
# maptype='terrain'
map1 <- get_map("daegu", zoom=7 ,  maptype='terrain')
map2 <- ggmap(map1)
map3 <- map2 + geom_point(aes(x=lon,y=lat,colour=house,size=house),data=df)
map3 + geom_text(data=df, aes(x=lon+0.01, y=lat+0.18,label=region),size=3)
# [실습] 다양한 지도 유형
# maptype='terrain'
map1 <- get_map("daegu", zoom=7 ,  maptype='terrain')
map2 <- ggmap(map1)
# 2015년도 06월 기준 대한민국 인구수
pop <- read.csv("C:/Rwork-I/Part-II/population201506.csv",header=T,
stringsAsFactors = F)
pop
# 2015년도 06월 기준 대한민국 인구수
pop <- read.csv("C:/Rwork-I/Part-II/population201506.csv",header=T,
stringsAsFactors = F)
# 2015년도 06월 기준 대한민국 인구수
pop <- read.csv("C:/Rwork/Part-II/population201506.csv",header=T,
stringsAsFactors = F)
pop
library(stringr)
region <- pop$지역명
lon <- pop$LON # 경도
lat <- pop$LAT # 위도
house <- as.numeric(str_replace_all(pop$세대수, ',',''))
house
# 위도,경도,세대수 이용 데이터프레임 생성
df <- data.frame(region, lon,lat,house)
df
# [실습] 다양한 지도 유형
# maptype='terrain'
map1 <- get_map("daegu", zoom=7 ,  maptype='terrain')
map2 <- ggmap(map1)
map3 <- map2 + geom_point(aes(x=lon,y=lat,colour=house,size=house),data=df)
map3 + geom_text(data=df, aes(x=lon+0.01, y=lat+0.18,label=region),size=3)
# maptype='satellite'
map1 <- get_map("daegu", zoom=7 ,  maptype='satellite')
map2 <- ggmap(map1)
map3 <- map2 + geom_point(aes(x=lon,y=lat,colour=house,size=house),data=df)
map3 + geom_text(data=df, aes(x=lon+0.01,y=lat+0.18,colour=region,label=region),size=3)
# maptype='satellite'
map1 <- get_map("daegu", zoom=7 ,  maptype='satellite')
map2 <- ggmap(map1)
map3 <- map2 + geom_point(aes(x=lon,y=lat,colour=house,size=house),data=df)
map3 + geom_text(data=df, aes(x=lon+0.01,y=lat+0.18,colour=region,label=region),size=3)
# maptype='roadmap'
map1 <- get_map("daegu", zoom=7 ,  maptype='roadmap')
map2 <- ggmap(map1)
map3 <- map2 + geom_point(aes(x=lon,y=lat,colour=house,size=house),data=df)
map3 + geom_text(data=df, aes(x=lon+0.01, y=lat+0.18,label=region),size=3)
# maptype='hybrid'
map1 <- get_map("jeonju", zoom=7,  maptype='hybrid')
map2 <- ggmap(map1)
map3 <- map2 + geom_point(aes(x=lon,y=lat,colour=house,size=house),data=df)
map3 + geom_text(data=df, aes(x=lon+0.01, y=lat+0.18,label=region),size=3)
map3 + geom_density2d()
map1 <- get_map("daegu", zoom=7 ,  maptype='satellite')
map2 <- ggmap(map1)
map3 <- map2 + geom_point(aes(x=lon,y=lat,colour=house,size=house),data=df)
map3 + geom_text(data=df, aes(x=lon+0.01,y=lat+0.18,colour=region,label=region),size=3)
map1 <- get_map("daegu", zoom=7 ,  maptype='satellite')
map2 <- ggmap(map1)
map3 <- map2 + geom_point(aes(x=lon,y=lat,colour=house,size=house),data=df)
map3 + geom_text(data=df, aes(x=lon+0.01, y=lat+0.18,label=region),size=3)
map1 <- get_map("daegu", zoom=7 ,  maptype='terrain')
map2 <- ggmap(map1)
map3 <- map2 + geom_point(aes(x=lon,y=lat,colour=house,size=house),data=df)
map3 + geom_text(data=df, aes(x=lon+0.01, y=lat+0.18,label=region),size=3)
# chap09_2_InFormal
########################################
## Chapter09_2. 비정형데이터 처리
########################################
# 2.1 토픽 분석
# 텍스트마이닝을 위한 패키지 설치
#install.packages("rJava")
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_151')
library(rJava) # 로딩
# 토픽 분석을 위한 패키지 설치
#install.packages(c("KoNLP", "wordcloud"))
# tm 패키지 구 버전 다운로드/설치 - version 3.3.2
#install.packages("http://cran.r-project.org/bin/windows/contrib/3.0/tm_0.5-10.zip",repos=NULL)
#install.packages('slam')
library(slam)
library(tm) # tm 패키지는 slam 패키지에 의존적임
# 패키지 로딩
library(KoNLP) # 세종사전
library(tm) # 영문 텍스트 마이닝
library(wordcloud) # RColorBrewer()함수 제공
# [실습] 명사 추출 예
extractNoun('안녕하세요. 홍길동 입니다.')
extractNoun('텍스트마이닝이란 텍스트에서 유용한 정보를 찾아내는 과정을 말한다')
# [실습] 텍스트 자료 가져오기
facebook <- file("C:/Rwork/Part-II/facebook_bigdata.txt", encoding="UTF-8")
facebook_data <- readLines(facebook) # 줄 단위 데이터 생성
head(facebook_data) # 앞부분 6줄 보기 - 줄 단위 문장 확인
str(facebook_data) # chr [1:76]
# [실습] 자료집(Corpus) 생성
facebook_corpus <- Corpus(VectorSource(facebook_data))
facebook_corpus
inspect(facebook_corpus) # 76개 자료집에 포함된 문자 수 제공
# [실습] 단어 추가와 단어추출
#  (1) 세종 사전 사용 및 단어 추가
#install.packages('curl')
library(curl)
useSejongDic() # 세종 사전 불러오기
# (2) 세종 사전에 없는 단어 추가
mergeUserDic(data.frame(c("R 프로그래밍","페이스북","소셜네트워크"), c("ncn")))
# ncn -명사지시코드
# [실습] 단어추출 사용자 함수 정의
# (1) 사용자 정의 함수 작성
exNouns <- function(x) { paste(extractNoun(as.character(x)), collapse=" ")}
# (2) exNouns 함수 이용 단어 추출
facebook_nouns <- sapply(facebook_corpus, exNouns)
facebook_nouns[1] # 단어만 추출된 첫 줄 보기
# [실습] 추출된 단어 대상 전처리
# (1) 추출된 단어 이용 자료집 생성
myCorputfacebook <- Corpus(VectorSource(facebook_nouns))
myCorputfacebook
# (2) 데이터 전처리
myCorputfacebook <- tm_map(myCorputfacebook, removePunctuation) # 문장부호 제거
myCorputfacebook <- tm_map(myCorputfacebook, removeNumbers) # 수치 제거
myCorputfacebook <- tm_map(myCorputfacebook, tolower) # 소문자 변경
myCorputfacebook <-tm_map(myCorputfacebook, removeWords, stopwords('english')) # 불용어제거
inspect( myCorputfacebook[1:5])
# [실습] 단어 선별(단어 길이 2개 이상)
# 전처리된 단어집을 대상으로 일반문서로 변환
myCorputfacebook_txt <- tm_map(myCorputfacebook, PlainTextDocument)
# 단어길이 2개 이상인 단어만 선별하여 matrix 자료구조로 변경
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt, control=list(wordLengths=c(2,Inf)))
myCorputfacebook_txt
# matrix 자료구조를 data.frame 자료구조로 변경
myTermfacebook.df <- as.data.frame(as.matrix(myCorputfacebook_txt))
dim(myTermfacebook.df)
# [실습] 단어 빈도수 구하기 - 빈도수가 높은 순서대로 내림차순 정렬
wordResult <- sort(rowSums(myTermfacebook.df), decreasing=TRUE) # 빈도수로 내림차순 정렬
wordResult[1:10]
#  [실습] 단어 구름(wordcloud) 생성 - 디자인 적용 전
myName <- names(wordResult) # 단어 이름 생성 -> 빈도수의 이름
wordcloud(myName, wordResult) # 단어구름 적성
# [실습] 불필요한 단어 제거 시작
# 1) 데이터 전처리
myCorputfacebook <- tm_map(myCorputfacebook, removePunctuation) # 문장부호 제거
myCorputfacebook <- tm_map(myCorputfacebook, removeNumbers) # 수치 제거
myCorputfacebook <- tm_map(myCorputfacebook, tolower) # 소문자 변경
myStopwords = c(stopwords('english'), "사용", "하기");
myCorputfacebook = tm_map(myCorputfacebook, removeWords, myStopwords);
inspect(myCorputfacebook[1:5]) # 데이터 전처리 결과 확인
# 2) 단어 선별 - 단어 길이 2개 이상 단어 선별
myCorputfacebook_txt <- tm_map(myCorputfacebook, PlainTextDocument)
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt, control=list(wordLengths=c(2,Inf)))
myCorputfacebook_txt
# matrix -> data.frame 변경
myTermfacebook.df <- as.data.frame(as.matrix(myCorputfacebook_txt))
dim(myTermfacebook.df) # [1] 876  76
# 3) 단어 빈도수 구하기
wordResult <- sort(rowSums(myTermfacebook.df), decreasing=TRUE) # 빈도수로 내림차순 정렬
wordResult[1:10]
# 4) 단어 구름(wordcloud) 생성  생성 - 디자인 적용 전
myName <- names(wordResult) # 단어 이름 추출(빈도수 이름)
wordcloud(myName, wordResult) # 단어구름 시각화
# [실습] 단어 구름에 디자인 적용(빈도수, 색상, 위치, 회전 등)
# (1) 단어이름과 빈도수로 data.frame 생성
word.df <- data.frame(word=myName, freq=wordResult)
str(word.df) # word, freq 변수
# (2) 단어 색상과 글꼴 지정
pal <- brewer.pal(12,"Paired") # 12가지 색상 pal <- brewer.pal(9,"Set1") # Set1~ Set3
# 폰트 설정세팅 : "맑은 고딕", "서울남산체 B"
windowsFonts(malgun=windowsFont("맑은 고딕"))  #windows
# (3) 단어 구름 시각화 - 별도의 창에 색상, 빈도수, 글꼴, 회전 등의 속성 적용
x11( ) # 별도의 창을 띄우는 함수
wordcloud(word.df$word, word.df$freq,
scale=c(5,1), min.freq=3, random.order=F,
rot.per=.1, colors=pal, family="malgun")
## 2.2 연관어 분석
# [실습] 한글 처리를 위한 패키지 설치
#install.packages('rJava')
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_151')
library(rJava) # 아래와 같은 Error 발생 시 Sys.setenv()함수로 java 경로 지정
#install.packages("KoNLP")
library(KoNLP) # rJava 라이브러리가 필요함
# [실습] 텍스트 파일 가져오기와 단어 추출하기
# 1.텍스트 파일 가져오기
marketing <- file("C:/Rwork/Part-II/marketing.txt", encoding="UTF-8")
marketing2 <- readLines(marketing) # 줄 단위 데이터 생성
# incomplete final line found on - Error 발생 시 UTF-8 인코딩 방식으로 재 저장
close(marketing)
# 2. 줄 단위 단어 추출
lword <- Map(extractNoun, marketing2)
length(lword) # [1] key = 472
lword <- unique(lword) # 중복제거1(전체 대상)
length(lword) # [1] 353(19개 제거)
# 3. 중복단어 제거와 추출 단어 확인
lword <- sapply(lword, unique) # 중복제거2(줄 단위 대상)
length(lword) # [1] 352(1개 제거)
# [실습] 연관어 분석을 위한 전처리
# 1) 단어 필터링 함수 정의
filter1 <- function(x){
nchar(x) <= 4 && nchar(x) >= 2 && is.hangul(x)
}
filter2 <- function(x){
Filter(filter1, x)
}
# 2) 줄 단위로 추출된 단어 전처리
lword <- sapply(lword, filter2)
lword
# [실습] 필터링 간단 예문
# 1) vector 이용 list 객체 생성
word <- list(c("홍길동","이순","만기","김"),
c("대한민국","우리나라대한민국","한국","resu"))   # 영문자 포함
class(word)
# 2) 단어 필터링 함수 정의(길이 2~4 사이인 한글 단어추출)
filter1 <- function(x) {
nchar(x) <= 4 && nchar(x) >= 2 && is.hangul(x)
}
filter2 <- function(x) {
Filter(filter1, x)
}
# 3) 함수 적용 list 객체 필터링
filterword <- sapply(word, filter2)
filterword
# [실습] 트랜잭션 생성
# 1) 연관분석을 위한 패키지 설치
install.packages("arules")
library(arules)
# 2) 트랜잭션 생성
wordtran <- as(lword, "transactions") # lword에 중복데이터가 있으면 error발생
wordtran
# [실습] 단어 간 연관규칙 산출
tranrules <- apriori(wordtran, parameter=list(supp=0.25, conf=0.05))
# [실습] 연관규칙 생성 결과보기
inspect(tranrules) # 연관규칙 생성 결과(59개) 보기
# [실습] 연관규칙 생성 간단 예문
data(Adult)
Adult
str(Adult)
dim(Adult)   # 차원보기 : 48,842개 트랜잭션과 115개 아이템
inspect(Adult)
data(Adult)
Adult
str(Adult)
dim(Adult)   # 차원보기 : 48,842개 트랜잭션과 115개 아이템
inspect(Adult)
library(arules)
# 2) 트랜잭션 생성
wordtran <- as(lword, "transactions") # lword에 중복데이터가 있으면 error발생
wordtran
# [실습] 단어 간 연관규칙 산출
tranrules <- apriori(wordtran, parameter=list(supp=0.25, conf=0.05))
# [실습] 연관규칙 생성 결과보기
inspect(tranrules) # 연관규칙 생성 결과(59개) 보기
rules <- labels(tranrules, ruleSep=" ")
rules
class(rules)
# 2) 문자열로 묶인 연관단어를 행렬구조 변경
rules <- sapply(rules, strsplit, " ",  USE.NAMES=F)
rules
class(rules)
# 3) 행 단위로 묶어서 matrix로 반환
rulemat <- do.call("rbind", rules)
rulemat
class(rulemat)
# [실습] 연관어 시각화를 위한 igraph 패키지 설치
#install.packages("igraph") # graph.edgelist(), plot.igraph(), closeness() 함수 제공
library(igraph)
# [실습] edgelist보기 - 연관단어를 정점 형태의 목록 제공
ruleg <- graph.edgelist(rulemat[c(12:59),], directed=F) # [1,]~[11,] "{}" 제외
ruleg
# [실습] edgelist 시각화
X11()
plot.igraph(ruleg, vertex.label=V(ruleg)$name,
vertex.label.cex=1.2, vertex.label.color='black',
vertex.size=20, vertex.color='green', vertex.frame.color='blue')
# [실습] 연관규칙 생성 간단 예문
data(Adult)
Adult
str(Adult)
dim(Adult)   # 차원보기 : 48,842개 트랜잭션과 115개 아이템
inspect(Adult)
apr1 <- apriori(Adult, parameter = list(support= 0.1, target="frequent"),
appearance = list(none = c("income=small", "income=large"),
default="both"))
apr1
inspect(apr1)
apr2 <- apriori(Adult, parameter = list(support= 0.1, target="rules"),
appearance = list(none = c("income=small", "income=large"),
default="both"))
apr3 <- apriori(Adult, parameter = list(support= 0.5, conf = 0.9, target="rules"),
appearance = list(none = c("income=small", "income=large"),
default="both"))
# chap14_1_Factor Analysis
###########################################
# Chapter14_1. 요인분석(Factor Analysis)
###########################################
# 요인분석의 목적
# 1. 자료의 요약 : 변인을 몇 개의 공통된 변인으로 묶음
# 2. 변인 구조 파악 : 변인들의 상호관계 파악(독립성 등)
# 3. 불필요한 변인 제거 : 중요도가 떨어진 변수 제거
# 4. 측정도구 타당성 검증 : 변인들이 동일한 용인으로 묶이는지 여부를 확인
# 전제조건 : 등간척도 or 비율척도, 정규분포, 관찰치 상호독립적/분산 동일
# 요인분석 결과에 대한 활용 방안
# 1. 서로 밀접하게 관련된 변수들을 합치거나 중복된 변수를 제거하여 변수를 축소한다.
# 2. 변수들 간의 연관성 또는 공통점 탐색
# 3. 요인점수 계산으로 상관분석, 회귀분석의 설명변수로 이용
## 1.1 공통요인으로 변수 정제
# [실습] 변수와 데이터프레임 생성
s1 <- c(1, 2, 1, 2, 3, 4, 2, 3, 4, 5)
s2 <- c(1, 3, 1, 2, 3, 4, 2, 4, 3, 4)
s3 <- c(2, 3, 2, 3, 2, 3, 5, 3, 4, 2)
s4 <- c(2, 4, 2, 3, 2, 3, 5, 3, 4, 1)
s5 <- c(4, 5, 4, 5, 2, 1, 5, 2, 4, 3)
s6 <- c(4, 3, 4, 4, 2, 1, 5, 2, 4, 2)
name <-1:10
subject <- data.frame(s1, s2, s3, s4, s5, s6)
subject
str(subject)
summary(subject)
# [실습] 변수의 주요 성분분석
pc <- prcomp(subject) # scale = TRUE
summary(pc)
plot(pc)
# 고유값으로 요인 수 분석
en <- eigen(cor(subject)) # $values : 고유값, $vectors : 고유벡터
names(en) # "values"  "vectors"
en$values
en$vectors
en$values # $values : 고유값(스칼라) 보기
plot(en$values, type="o") # 고유값을 이용한 시각화
# [실습] 변수 간의 상관관계 분석과 요인분석
cor(subject)
# 요인분석 : 요인회전법 적용(varimax is the default)
# (1) 주성분분석의 가정에 의해서 2개 요인으로 분석
result <- factanal(subject, factors = 2, rotation = "varimax")
result # p-value is 0.0232  < 0.05
# (2) 고유값으로 가정한 3개 요인으로 분석
result <- factanal(subject, factors = 3, # 요인 개수 지정
rotation = "varimax", # 회전방법 지정("varimax", "promax", "none")
scores="regression") # 요인점수 계산 방법
result
# 요인적재량 보기
names(result)
result$loadings
# (3) 다양한 방법으로 요인적재량 보기
print(result, digits = 2, cutoff=0.5)
print(result$loadings, cutoff=0) # display every loadings
# 요인점수 보기 : 관측치의 동작을 살펴보는데 사용되며, 상관분석이나 회귀분석의 독립변수로 사용
# 표본의 변수값들을 구해진 요인들의 값(요인적재량)으로 변경해 준 것을 요인점수라고 한다.
# 각 변수(표준화값)와 요인 간의 관계(요인부하량)를 통해서 구해진 점수
result$scores
# [실습] 요인점수를 이용한 요인적재량 시각화
# (1) Factor1, Factor2 요인지표 시각화
plot(result$scores[, c(1:2)], main="Factor1과 Factor2 요인점수 행렬")
text(result$scores[, 1], result$scores[,2],
labels = name, cex = 0.7, pos = 3, col = "blue")
# 요인적재량 plotting
points(result$loadings[, c(1:2)], pch=19, col = "red")
text(result$loadings[, 1], result$loadings[,2],
labels = rownames(result$loadings),
cex = 0.8, pos = 3, col = "red")
# (2) Factor1, Factor3 요인지표 시각화
plot(result$scores[,c(1,3)], main="Factor1과 Factor3 요인점수 행렬")
text(result$scores[,1], result$scores[,3],
labels = name, cex = 0.7, pos = 3, col = "blue")
# 요인적재량 plotting
points(result$loadings[,c(1,3)], pch=19, col = "red")
text(result$loadings[,1], result$loadings[,3],
labels = rownames(result$loadings),
cex = 0.8, pos = 3, col = "red")
# [실습] 3차원 산점도로 요인적재량 시각화
library(scatterplot3d)
Factor1 <- result$scores[,1]
Factor2 <- result$scores[,2]
Factor3 <- result$scores[,3]
# scatterplot3d(밑변, 오른쪽변, 왼쪽변, type='p') # type='p' : 기본산점도 표시
d3 <- scatterplot3d(Factor1, Factor2, Factor3)
# 요인적재량 표시
loadings1 <- result$loadings[,1]
loadings2 <- result$loadings[,2]
loadings3 <- result$loadings[,3]
d3$points3d(loadings1, loadings2, loadings3, bg='red',pch=21, cex=2, type='h')
# [실습] 요인별 변수 묶기
# (1) 요인별 과목변수 이용 데이터프레임 생성
app <- data.frame(subject$s5, subject$s6) # 응용과학
soc <- data.frame(subject$s3, subject$s4) # 사회과학
net <- data.frame(subject$s1, subject$s2) # 자연과학
# (2) 산술평균 계산
app_science <- round( (app$subject.s5 + app$subject.s6) / ncol(app), 2)
soc_science <- round( (soc$subject.s3 + soc$subject.s4) / ncol(soc), 2)
net_science <- round( (net$subject.s1 + net$subject.s2) / ncol(net), 2)
# (3) 상관관계 분석
subject_factor_df <- data.frame(app_science, soc_science, net_science)
cor(subject_factor_df)
## 1.2 잘못 분류된 요인 제거로 변수 정제
# [실습] 요인분석에 사용될 데이터 셋 가져오기
# (1) 데이터 가져오기
#install.packages('memisc')
library(memisc)
setwd("C:\\Rwork\\Part-III")
data.spss <- as.data.set(spss.system.file('drinking_water.sav'))
data.spss
# (2) 데이터프레임으로 변경
drinking_water <- data.spss[1:11]
drinking_water
drinking_water_df <- as.data.frame(drinking_water)
str(drinking_water_df)
# (3)  요인수를 3개로 지정하여 요인분석 수행
result2 <- factanal(drinking_water_df, factors = 3, rotation = "varimax",
scores = "regression")
result2
print(result2, cutoff=0.5)
# [실습] 요인별 변수 묶기
# 1)  q4 칼럼 제외하여 데이터프레임 생성
dw_df <- drinking_water_df[-4]
str(dw_df)
dim(dw_df) # [1] 380  10
cor(dw_df)
# 2) 요인에 속하는 입력변수별 데이터프레임 구성
# 제품만족도 저장 데이터프레임
s <- data.frame(dw_df$q8, dw_df$q9, dw_df$q10, dw_df$q11)
# 제품친밀도 저장 데이터프레임
c <- data.frame(dw_df$q1, dw_df$q2, dw_df$q3)
# 제품적절성 저장 데이터프레임
p <- data.frame(dw_df$q5, dw_df$q6, dw_df$q7)
# 3) 요인별 산술평균 계산
satisfaction <- round( (s$dw_df.q8 + s$dw_df.q9 + s$dw_df.q10 + s$dw_df.q11) / ncol(s), 2)
closeness <- round( (c$dw_df.q1 + c$dw_df.q2 + c$dw_df.q3) / ncol(c), 2)
pertinence <- round( (p$dw_df.q5 + p$dw_df.q6 + p$dw_df.q7) / ncol(p), 2)
# 4) 상관관계 분석
drinking_water_factor_df <- data.frame(satisfaction, closeness, pertinence)
colnames(drinking_water_factor_df) <- c("제품만족도","제품친밀도","제품적절성")
cor(drinking_water_factor_df)
# chap14_2_Correlation Analysis
######################################################
## Chapter14_2. 상관관계 분석(Correlation Analysis)
######################################################
# 2.2 상관관계 분석 수행
# [실습] 기술 통계량 구하기
result <- read.csv("C:/Rwork/Part-III/product.csv", header=TRUE)
head(result) # 친밀도 적절성 만족도(등간척도 - 5점 척도)
# 기술통계량
summary(result) # 요약통계량
sd(result$제품_친밀도); sd(result$제품_적절성); sd(result$제품_만족도)
# [실습] 상관계수(coefficient of correlation) : 두 변량 X,Y 사이의 상관관계 정도를 나타내는 수치(계수)
cor(result$제품_친밀도, result$제품_적절성) # 0.4992086 -> 다소 높은 양의 상관관계
cor(result$제품_친밀도, result$제품_만족도) # 0.467145 -> 다소 높은 양의 상관관계
# [실습] 전체 변수 간 상관계수 보기
cor(result, method="pearson")
# [실습] 방향성 있는 색생으로 표현
#install.packages("corrgram")
library(corrgram)
corrgram(result) # 색상 적용 - 동일 색상으로 그룹화 표시
corrgram(result, upper.panel=panel.conf) # 수치(상관계수) 추가(위쪽)
corrgram(result, lower.panel=panel.conf) # 수치(상관계수) 추가(아래쪽)
# [실습] 차트에 밀도 곡선, 상관성, 유의확률(별표) 추가
#install.packages("PerformanceAnalytics")
library(PerformanceAnalytics)
# 상관성,p값(*),정규분포 시각화 - 모수 검정 조건
chart.Correlation(result, histogram=, pch="+")
# [실습]  spearman : 서열척도 대상 상관계수
cor(result, method="spearman")

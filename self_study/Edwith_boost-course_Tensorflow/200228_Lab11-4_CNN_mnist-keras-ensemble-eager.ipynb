{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "200228_Lab11-4_CNN_mnist-keras-ensemble-eager.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IZ30XA5PBRa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "2e394180-8e01-4caa-a5ac-6afc729d9fef"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import time\n",
        "\n",
        "print(tf.__version__)\n",
        "print(keras.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0\n",
            "2.2.4-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX2H_MrxPBSN",
        "colab_type": "text"
      },
      "source": [
        "## 1. Set hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3S99zqcPBSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.001\n",
        "train_epoch = 15\n",
        "bsize = 100\n",
        "\n",
        "tf.random.set_seed(777)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jYe0WLhPBSg",
        "colab_type": "text"
      },
      "source": [
        "#### + Create checkpoint directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t00sVzwgPBSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cur_dir = os.getcwd()\n",
        "ckpt_dir_name = 'checkpoints'\n",
        "model_dir_name = 'mnist_cnn_ensemble'\n",
        "\n",
        "checkpoint_dir = os.path.join(cur_dir, ckpt_dir_name, model_dir_name)\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "ckpt_prefix = os.path.join(checkpoint_dir, model_dir_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEOumqMcPBSy",
        "colab_type": "text"
      },
      "source": [
        "## 2. Make a data pipelining"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VOy-SUzPBS3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "b64ed28b-305d-41eb-f01f-5ee15c3332f0"
      },
      "source": [
        "mnist = keras.datasets.mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.astype(np.float32) / 255.\n",
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "\n",
        "X_test = X_test.astype(np.float32) / 255.\n",
        "X_test = np.expand_dims(X_test, axis=-1)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).\\\n",
        "                    shuffle(buffer_size=100000).batch(batch_size=bsize)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).\\\n",
        "                    batch(batch_size=bsize)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jH2lXkvRPBTI",
        "colab_type": "text"
      },
      "source": [
        "## 3. Build a neural network model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZKxwvZQPBTM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNISTModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(MNISTModel, self).__init__()\n",
        "        self.conv1 = keras.layers.Conv2D(filters=32, kernel_size=3,\n",
        "                                         strides=1, padding='same',\n",
        "                                         activation=tf.nn.relu)\n",
        "        self.pool1 = keras.layers.MaxPool2D(pool_size=2, padding='same',\n",
        "                                            strides=2)\n",
        "        self.conv2 = keras.layers.Conv2D(filters=64, kernel_size=3,\n",
        "                                         strides=1, padding='same',\n",
        "                                         activation=tf.nn.relu)\n",
        "        self.pool2 = keras.layers.MaxPool2D(pool_size=2, padding='same',\n",
        "                                            strides=2)\n",
        "        self.conv3 = keras.layers.Conv2D(filters=128, kernel_size=3,\n",
        "                                         strides=1, padding='same',\n",
        "                                         activation=tf.nn.relu)\n",
        "        self.pool3 = keras.layers.MaxPool2D(pool_size=2, padding='same',\n",
        "                                            strides=2)\n",
        "        self.pool3_flat = keras.layers.Flatten()\n",
        "        \n",
        "        self.dense4 = keras.layers.Dense(units=256, activation=tf.nn.relu)\n",
        "        self.drop4 = keras.layers.Dropout(rate=0.4)\n",
        "        self.dense5 = keras.layers.Dense(units=10)\n",
        "        pass\n",
        "    \n",
        "    def call(self, inputs, training=False):\n",
        "        net = self.conv1(inputs)\n",
        "        net = self.pool1(net)\n",
        "        net = self.conv2(net)\n",
        "        net = self.pool2(net)\n",
        "        net = self.conv3(net)\n",
        "        net = self.pool3(net)\n",
        "        net = self.pool3_flat(net)\n",
        "        \n",
        "        net = self.dense4(net)\n",
        "        net = self.drop4(net)\n",
        "        net = self.dense5(net)\n",
        "        return net\n",
        "\n",
        "models = []\n",
        "num_models = 3\n",
        "for i in range(num_models):\n",
        "    model = MNISTModel()\n",
        "    models.append(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iapfqzn3PBTk",
        "colab_type": "text"
      },
      "source": [
        "## 4. Define a loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l1KPP9mPBTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_fn(model, images, labels):\n",
        "    logits = model(images, training=True)\n",
        "    loss = tf.reduce_mean( \n",
        "                keras.losses.categorical_crossentropy(\n",
        "                    y_true=labels, y_pred=logits, from_logits=True))\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RR4ELoNrPBT6",
        "colab_type": "text"
      },
      "source": [
        "## 5. Calculate a gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqNOui2fPBT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def grad(model, images, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = loss_fn(model, images, labels)\n",
        "    return tape.gradient( loss, model.variables )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka7fh9UlPBUK",
        "colab_type": "text"
      },
      "source": [
        "## 6. Select an optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8oM1X-xPBUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm0cePUJPBUl",
        "colab_type": "text"
      },
      "source": [
        "## 7. Define a metric for model's performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lt-_gF5hPBUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(models, images, labels):\n",
        "    predictions = np.zeros_like(labels)\n",
        "    for model in models:\n",
        "        logits = model(images, training=False)\n",
        "        predictions += logits\n",
        "    correct_prediction = tf.equal( x=tf.argmax(predictions, 1),\n",
        "                                   y=tf.argmax(labels, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))    \n",
        "    return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b3240fdPBU-",
        "colab_type": "text"
      },
      "source": [
        "## 8. Make a checkpoint for saving"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrtzIi5ZPBVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoints = []\n",
        "for i in range(num_models):\n",
        "    checkpoints.append(tf.train.Checkpoint(cnn=models[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnIkyDqvPBVV",
        "colab_type": "text"
      },
      "source": [
        "## 9. Train and Validate a neural network model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJqUw3a9PBVZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "2f78cfcd-c722-4071-c0e8-1b42e8f4c6f4"
      },
      "source": [
        "print('Learning started. It takes some time.')\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(train_epoch):\n",
        "  avg_loss = 0.; avg_train_acc = 0.; avg_test_acc = 0.\n",
        "  train_step = 0; test_step = 0\n",
        "  \n",
        "  for images, labels in train_dataset:\n",
        "    for model in models:\n",
        "      grads = grad(model, images, labels)\n",
        "      optimizer.apply_gradients(zip(grads, model.variables))\n",
        "      \n",
        "      loss = loss_fn(model, images, labels)\n",
        "      avg_loss += loss / num_models\n",
        "    acc = evaluate(models, images, labels)\n",
        "    avg_train_acc += acc / num_models\n",
        "    train_step += 1\n",
        "  avg_loss = avg_loss / train_step\n",
        "  avg_train_acc = avg_train_acc / train_step\n",
        "\n",
        "  for images, labels in test_dataset:\n",
        "    for model in models:\n",
        "      acc = evaluate(models, images, labels)\n",
        "      avg_test_acc += acc\n",
        "      test_step += 1\n",
        "    avg_test_acc = avg_test_acc / test_step\n",
        "  \n",
        "  print(f'Epoch: {epoch+1:3}  |  loss = {avg_loss:10.8f}  |  ' +\n",
        "        f'train_accuracy = {avg_train_acc:6.4f}  |  ' +\n",
        "        f'test_accuracy = {avg_test_acc:6.4f}')\n",
        "\n",
        "  for idx, checkpoint in enumerate(checkpoints):\n",
        "    checkpoint.save(ckpt_prefix + f' - {idx}')\n",
        "\n",
        "print('Learning Finished !')\n",
        "print(f'time elapsed : {(time.time() - start_time)/60:.4f} min.')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning started. It takes some time.\n",
            "Epoch:   1  |  loss = 0.16240056  |  train_accuracy = 0.3218  |  test_accuracy = 0.0099\n",
            "Epoch:   2  |  loss = 0.03939417  |  train_accuracy = 0.3311  |  test_accuracy = 0.0100\n",
            "Epoch:   3  |  loss = 0.02686219  |  train_accuracy = 0.3319  |  test_accuracy = 0.0100\n",
            "Epoch:   4  |  loss = 0.01978362  |  train_accuracy = 0.3324  |  test_accuracy = 0.0100\n",
            "Epoch:   5  |  loss = 0.01597871  |  train_accuracy = 0.3326  |  test_accuracy = 0.0100\n",
            "Epoch:   6  |  loss = 0.01269017  |  train_accuracy = 0.3329  |  test_accuracy = 0.0100\n",
            "Epoch:   7  |  loss = 0.01105772  |  train_accuracy = 0.3330  |  test_accuracy = 0.0100\n",
            "Epoch:   8  |  loss = 0.00889889  |  train_accuracy = 0.3331  |  test_accuracy = 0.0100\n",
            "Epoch:   9  |  loss = 0.00810749  |  train_accuracy = 0.3332  |  test_accuracy = 0.0100\n",
            "Epoch:  10  |  loss = 0.00771566  |  train_accuracy = 0.3331  |  test_accuracy = 0.0100\n",
            "Epoch:  11  |  loss = 0.00594616  |  train_accuracy = 0.3332  |  test_accuracy = 0.0100\n",
            "Epoch:  12  |  loss = 0.00590546  |  train_accuracy = 0.3332  |  test_accuracy = 0.0100\n",
            "Epoch:  13  |  loss = 0.00465681  |  train_accuracy = 0.3333  |  test_accuracy = 0.0100\n",
            "Epoch:  14  |  loss = 0.00508001  |  train_accuracy = 0.3333  |  test_accuracy = 0.0100\n",
            "Epoch:  15  |  loss = 0.00425480  |  train_accuracy = 0.3333  |  test_accuracy = 0.0100\n",
            "Learning Finished !\n",
            "time elapsed : 10.4472 min.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
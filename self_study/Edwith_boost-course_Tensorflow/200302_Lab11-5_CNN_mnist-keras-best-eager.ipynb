{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "200302_Lab11-5_CNN_mnist-keras-best-eager.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2mKfePQSsDw",
        "colab_type": "code",
        "outputId": "c8fa8bfc-a59c-454d-ed38-e3f7be21c78d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# !pip install --upgrade tensorflow\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from scipy import ndimage\n",
        "import time\n",
        "\n",
        "print(tf.__version__)\n",
        "print(keras.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0\n",
            "2.2.4-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfpBYEv6Ssha",
        "colab_type": "text"
      },
      "source": [
        "## 1. Set hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7wFncFISXzj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.001\n",
        "train_epoch = 12\n",
        "bsize = 80\n",
        "\n",
        "tf.random.set_seed(777)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LBqpEkTuUHyd"
      },
      "source": [
        "#### + Create a checkpoint directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2t_X0_HUPCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cur_dir = os.getcwd()\n",
        "ckpt_dir_name = 'checkpoints'\n",
        "model_dir_name = 'mnist_cnn_best'\n",
        "\n",
        "ckpt_dir = os.path.join(cur_dir, ckpt_dir_name, model_dir_name)\n",
        "ckpt_prefix = os.path.join(ckpt_dir_name, model_dir_name)\n",
        "\n",
        "os.makedirs(ckpt_dir, exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XQ_xZsGMSxyd"
      },
      "source": [
        "## 2. Data Augmentation (rotate & shift)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4ocRho1SYU0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_augmentation(images, labels):\n",
        "  aug_images = []; aug_labels = []\n",
        "\n",
        "  for image, label in zip(images, labels):\n",
        "    aug_images.append(image)\n",
        "    aug_labels.append(label)\n",
        "    \n",
        "    bg_value = np.median(image)\n",
        "    for _ in range(4):\n",
        "      angle = np.random.randint(-15, 15, 1)\n",
        "      rot_image = ndimage.rotate(image, angle[0], reshape=False, cval=bg_value)\n",
        "\n",
        "      shift = np.random.randint(-2, 2, 2)\n",
        "      shift_image = ndimage.shift(rot_image, shift, cval=bg_value)\n",
        "      \n",
        "      aug_images.append(shift_image)\n",
        "      aug_labels.append(label)\n",
        "\n",
        "  aug_images = np.array(aug_images)\n",
        "  aug_labels = np.array(aug_labels)\n",
        "  return aug_images, aug_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2NKDvLfjSyPa"
      },
      "source": [
        "## 3. Make a data pipelining"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlwLzYe_SYqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist = keras.datasets.mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train, y_train = data_augmentation(X_train, y_train)\n",
        "\n",
        "X_train = X_train.astype(np.float32) / 255.\n",
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).\\\n",
        "                  shuffle(buffer_size=500000).batch(bsize)\n",
        "\n",
        "X_test = X_test.astype(np.float32) / 255.\n",
        "X_test = np.expand_dims(X_test, axis=-1)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).\\\n",
        "                  batch(bsize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5CjJMDvMSy55"
      },
      "source": [
        "## 4. Build a neural network model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aI32LzTVS0B7",
        "colab": {}
      },
      "source": [
        "class ConvBNRelu(tf.keras.Model):\n",
        "  def __init__(self, filters, kernel_size=3, strides=1, padding='same'):\n",
        "    super(ConvBNRelu, self).__init__()\n",
        "    self.conv = keras.layers.Conv2D(filters=filters, kernel_size=kernel_size,\n",
        "                                    strides=strides, padding=padding, \n",
        "                                    kernel_initializer='glorot_normal')\n",
        "    self.batchnorm = keras.layers.BatchNormalization()\n",
        "\n",
        "  def call(self, inputs, training=False):\n",
        "    layer = self.conv(inputs)\n",
        "    layer = self.batchnorm(layer)\n",
        "    layer = tf.nn.relu(layer)\n",
        "    return layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zA38Lw56bVTN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DenseBNRelu(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(DenseBNRelu, self).__init__()\n",
        "    self.dense = keras.layers.Dense(units=units, \n",
        "                                    kernel_initializer='glorot_normal')\n",
        "    self.batchnorm = keras.layers.BatchNormalization()\n",
        "  \n",
        "  def call(self, inputs, training=False):\n",
        "    layer = self.dense(inputs)\n",
        "    layer = self.batchnorm(layer)\n",
        "    layer = tf.nn.relu(layer)\n",
        "    return layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOzaxBIUbWB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNISTModel(keras.Model):\n",
        "  def __init__(self):\n",
        "    super(MNISTModel, self).__init__()\n",
        "    self.conv1 = ConvBNRelu(filters=32)\n",
        "    self.pool1 = keras.layers.MaxPool2D(pool_size=2, strides=1,\n",
        "                                        padding='same')\n",
        "    self.conv2 = ConvBNRelu(filters=64)\n",
        "    self.pool2 = keras.layers.MaxPool2D(pool_size=2, strides=1,\n",
        "                                        padding='same')\n",
        "    self.conv3 = ConvBNRelu(filters=128)\n",
        "    self.pool3 = keras.layers.MaxPool2D(pool_size=2, strides=1,\n",
        "                                        padding='same')\n",
        "    self.pool3_flat = keras.layers.Flatten()\n",
        "\n",
        "    self.dense4 = DenseBNRelu(units=256)\n",
        "    self.drop4 = keras.layers.Dropout(rate=0.4)\n",
        "    self.dense5 = keras.layers.Dense(units=10,\n",
        "                                     kernel_initializer='glorot_normal')\n",
        "  \n",
        "  def call(self, inputs, training=False):\n",
        "    net = self.conv1(inputs)\n",
        "    net = self.pool1(net)\n",
        "    net = self.conv2(net)\n",
        "    net = self.pool2(net)\n",
        "    net = self.conv3(net)\n",
        "    net = self.pool3(net)\n",
        "    net = self.pool3_flat(net)\n",
        "\n",
        "    net = self.dense4(net)\n",
        "    net = self.drop4(net)\n",
        "    net = self.dense5(net)\n",
        "    return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOXPLvQ-hYsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models = []\n",
        "num_models = 5\n",
        "for _ in range(num_models):\n",
        "  models.append(MNISTModel())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YV4Eq_o6TBuP"
      },
      "source": [
        "## 5. Define a loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vWrUvmwiTasm",
        "colab": {}
      },
      "source": [
        "def loss_fn(model, images, labels):\n",
        "  logits = model(images, training=True)\n",
        "  loss = tf.reduce_mean(\n",
        "      tf.keras.losses.categorical_crossentropy(y_true=labels, \n",
        "                                          y_pred=logits, from_logits=True))\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KqkgLsbSTB2p"
      },
      "source": [
        "## 6. Calculate a gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Rj9wYwBATbRH",
        "colab": {}
      },
      "source": [
        "def grad(model, images, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss = loss_fn(model, images, labels)\n",
        "  return tape.gradient( loss, model.trainable_variables )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H-DTf54lTCAc"
      },
      "source": [
        "## 7. Select optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7QDBSjeHTbuU",
        "colab": {}
      },
      "source": [
        "lr_decay = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=lr,\n",
        "                                                       decay_steps=X_train.shape[0]/bsize*num_models*5,\n",
        "                                                       decay_rate=0.5,\n",
        "                                                       staircase=True)\n",
        "optimizer = keras.optimizers.Adam(learning_rate=lr_decay)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "czgw6N-nTCIr"
      },
      "source": [
        "## 8. Define a metric for model's performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QAXBTLrDTcJD",
        "colab": {}
      },
      "source": [
        "def evaluate(models, images, labels):\n",
        "  predictions = np.zeros_like(labels)\n",
        "  for model in models:\n",
        "    logits = model(images, training=False)\n",
        "    predictions += logits\n",
        "  correct_predictions = tf.equal( tf.argmax(predictions, 1),\n",
        "                                  tf.argmax(labels, 1) )\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_predictions, dtype=tf.float32))\n",
        "  return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OH5Wce3MTCPR"
      },
      "source": [
        "## 9. Make a checkpoint for saving"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PnfMdWR8Tc4x",
        "colab": {}
      },
      "source": [
        "checkpoints = []\n",
        "for i in range(num_models):\n",
        "  checkpoints.append(tf.train.Checkpoint(cnn=models[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cWjJuPUQTSMQ"
      },
      "source": [
        "## 10. Train and Validate a neural network model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bg_UJo6xTANj",
        "outputId": "8685118f-ab7b-40fc-b739-93751f20b198",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "print(\"Learning started. It takes some time.\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(train_epoch):\n",
        "  avg_loss = 0.; avg_train_acc = 0.; avg_test_acc = 0.\n",
        "  train_step = 0; test_step = 0\n",
        "\n",
        "  for images, labels in train_dataset:\n",
        "    for model in models:\n",
        "      grads = grad(model, images, labels)\n",
        "      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "      \n",
        "      loss = loss_fn(model, images, labels)\n",
        "      avg_loss += loss / num_models\n",
        "    acc = evaluate(models, images, labels)\n",
        "    avg_train_acc += acc\n",
        "    train_step += 1\n",
        "  avg_loss = avg_loss / train_step\n",
        "  avg_train_acc = avg_train_acc / train_step\n",
        "\n",
        "  for images, labels in test_dataset:\n",
        "    acc = evaluate(models, images, labels)\n",
        "    avg_test_acc += acc\n",
        "    test_step += 1\n",
        "  avg_test_acc = avg_test_acc / test_step\n",
        "\n",
        "  print(f'Epoch: {epoch+1:3}  |  loss = {avg_loss:10.8f}  |  ' +\n",
        "        f'train_accuracy = {avg_train_acc:6.4f}  |  ' +\n",
        "        f'test_accuracy = {avg_test_acc:6.4f}')\n",
        "\n",
        "  for idx, checkpoint in enumerate(checkpoints):\n",
        "    checkpoint.save(file_prefix = ckpt_prefix+f'-{idx}')\n",
        "\n",
        "print('Learning Finished!')\n",
        "print(f'time elapsed : {(time.time() - start_time)/60:.4f} min.')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning started. It takes some time.\n",
            "Epoch:   1  |  loss = 0.05480709  |  train_accuracy = 0.9396  |  test_accuracy = 0.9938\n",
            "Epoch:   2  |  loss = 0.02539429  |  train_accuracy = 0.9959  |  test_accuracy = 0.9943\n",
            "Epoch:   3  |  loss = 0.01816603  |  train_accuracy = 0.9976  |  test_accuracy = 0.9960\n",
            "Epoch:   4  |  loss = 0.01358163  |  train_accuracy = 0.9986  |  test_accuracy = 0.9956\n",
            "Epoch:   5  |  loss = 0.01055219  |  train_accuracy = 0.9991  |  test_accuracy = 0.9962\n",
            "Epoch:   6  |  loss = 0.00614742  |  train_accuracy = 0.9996  |  test_accuracy = 0.9966\n",
            "Epoch:   7  |  loss = 0.00448895  |  train_accuracy = 0.9998  |  test_accuracy = 0.9962\n",
            "Epoch:   8  |  loss = 0.00382253  |  train_accuracy = 0.9998  |  test_accuracy = 0.9961\n",
            "Epoch:   9  |  loss = 0.00324192  |  train_accuracy = 0.9999  |  test_accuracy = 0.9964\n",
            "Epoch:  10  |  loss = 0.00286996  |  train_accuracy = 0.9999  |  test_accuracy = 0.9968\n",
            "Epoch:  11  |  loss = 0.00185576  |  train_accuracy = 1.0000  |  test_accuracy = 0.9972\n",
            "Epoch:  12  |  loss = 0.00149440  |  train_accuracy = 1.0000  |  test_accuracy = 0.9965\n",
            "Learning Finished!\n",
            "time elapsed : 304.1768 min.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}